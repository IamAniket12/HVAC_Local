{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN \n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.insert(0,'boptestGymService')\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import BoptestGymEnvRewardWeightCost, NormalizedActionWrapper, NormalizedObservationWrapper, SaveAndTestCallback,DiscretizedActionWrapper\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC,PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import torch\n",
    "import wandb\n",
    "import requests\n",
    "url = 'http://127.0.0.1:5000'\n",
    "# url=\"https://api.boptest.net\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "    \n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get('cost_tot', 0) or 0\n",
    "        pdih_tot = kpis.get('pdih_tot', 0) or 0\n",
    "        pele_tot = kpis.get('pele_tot', 0) or 0\n",
    "        tdis_tot = kpis.get('tdis_tot', 0) or 0\n",
    "        idis_tot = kpis.get('idis_tot', 0) or 0\n",
    "\n",
    "        objective = (\n",
    "            cost_tot +\n",
    "            4.25 * (pdih_tot + pele_tot) +\n",
    "            0.005 * tdis_tot +\n",
    "            0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "        try:\n",
    "            #use this one running on local server\n",
    "            kpis = requests.get(f'{self.url}/kpi').json()['payload']\n",
    "\n",
    "            #use this when running boptest server\n",
    "            # print(self.test_id)\n",
    "            # print(self.url)\n",
    "            # kpis = requests.get('{0}/kpi/{1}'.format(self.url,self.testid)).json()['payload']\n",
    "            # print(kpis)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching KPIs: {e}\")\n",
    "            return 0  # In case of error, return zero reward\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "        \n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "        print(\"prev\",self.objective_integrand)\n",
    "        print(\"curr\",current_objective)\n",
    "        print(\"reward\",reward)\n",
    "        self.objective_integrand = current_objective\n",
    "        \n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With weight and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "def check_gpu_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is available. GPU details:\")\n",
    "        print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"  Memory allocated: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB\")\n",
    "        print(f\"  Memory cached: {torch.cuda.memory_reserved(0) / 1e6:.2f} MB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "def train_PPO_with_callback(model_path=None,\n",
    "                            log_dir=os.path.join('results', 'PPO_AD1', 'Model6'),\n",
    "                            tensorboard_log=os.path.join('results', 'PPO_AD1', 'Model6')):\n",
    "    \"\"\"\n",
    "    Method to train a PPO agent using a callback to save the model periodically.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str, optional\n",
    "        Path to a pre-trained model. If provided, the model will be loaded and further trained.\n",
    "    log_dir : str\n",
    "        Directory where monitoring data and best-trained model are stored.\n",
    "    tensorboard_log : str\n",
    "        Path to directory to load tensorboard logs.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "      \n",
    "    excluding_periods = []\n",
    "    excluding_periods.append((173*24*3600, 266*24*3600))  # Summer period\n",
    "    env_config = {\n",
    "        'url': url,\n",
    "        'actions': ['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        'observations': {\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
    "            'LowerSetp[1]':(280.,310.),\n",
    "            'UpperSetp[1]':(280.,310.),\n",
    "            'UpperCO2[1]':(0,10000),\n",
    "        },\n",
    "        'predictive_period': 5*3600,\n",
    "        'scenario': {'electricity_price': 'highly_dynamic'},\n",
    "        'random_start_time': True,\n",
    "        'max_episode_length': 3*24*3600,\n",
    "        'step_period': 3600,\n",
    "        'log_dir': log_dir,\n",
    "        'excluding_periods': excluding_periods\n",
    "    }\n",
    "    env = BoptestGymEnvCustomReward(\n",
    "        url=url,\n",
    "        actions=['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        observations={\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
    "            'LowerSetp[1]':(280.,310.),\n",
    "            'UpperSetp[1]':(280.,310.),\n",
    "            'UpperCO2[1]':(0,10000)\n",
    "        },\n",
    "        predictive_period     = 5*3600,\n",
    "        scenario={'electricity_price': 'highly_dynamic'},\n",
    "        random_start_time=True,\n",
    "        max_episode_length=5*24*3600,\n",
    "        step_period=3600,\n",
    "        log_dir=tensorboard_log,\n",
    "        excluding_periods=excluding_periods\n",
    "    )\n",
    "    \n",
    "    # env = DiscretizedActionWrapper(env, n_bins_act=15)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    run=wandb.init(\n",
    "        entity=\"continuous_action\",\n",
    "        project=\"ppo-training\",\n",
    "        config={\n",
    "            'env': env_config,\n",
    "            'verbose': 1,\n",
    "            'gamma': 0.99,\n",
    "            'learning_rate': 3e-4,\n",
    "            'n_steps': 2048,\n",
    "            'batch_size': 64,\n",
    "            'n_epochs': 10,\n",
    "            'clip_range': 0.2,\n",
    "            'gae_lambda': 0.95,\n",
    "            'ent_coef': 0.01,\n",
    "            'device': device\n",
    "        },\n",
    "        dir=log_dir,\n",
    "        resume=\"allow\"\n",
    "    )\n",
    "    print(run.id)\n",
    "    env = Monitor(env=env, filename=os.path.join(log_dir, 'monitor.csv'))\n",
    "    \n",
    "    # Callback to save model every 2000 steps\n",
    "    callback = SaveAndTestCallback(check_freq=48,save_freq=500,env=env,log_dir=tensorboard_log)\n",
    "    \n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Check if CUDA is available and force GPU usage if possible\n",
    "    \n",
    "    \n",
    "    check_gpu_usage()\n",
    "\n",
    "    # Load existing model if model_path is given, else create a new one\n",
    "    if model_path and os.path.isfile(model_path):\n",
    "        model = PPO.load(model_path, env=env, tensorboard_log=tensorboard_log, device=device)\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        model.set_logger(new_logger)  # Reconfigure the logger to continue logging\n",
    "    else:\n",
    "        model = PPO(\n",
    "            'MlpPolicy', \n",
    "            env, \n",
    "            verbose=1, \n",
    "            gamma=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.2,\n",
    "            gae_lambda=0.95,\n",
    "            ent_coef=0.01,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            device=device\n",
    "        )\n",
    "        model.set_logger(new_logger)\n",
    "        print(\"Starting training from scratch.\")\n",
    "    \n",
    "    # Verify that the model is on the correct device\n",
    "    print(f\"Model is on device: {next(model.policy.parameters()).device}\")\n",
    "    \n",
    "    # Train the agent with the callback\n",
    "    model.learn(total_timesteps=1000000, callback=WandbCallback(verbose=1,model_save_freq=1000,model_save_path=\"Model6\",gradient_save_freq=100))\n",
    "    # Finish W&B logging\n",
    "    run.finish()\n",
    "    return env, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = None # Update this with the correct path if needed\n",
    "    env, model = train_PPO_with_callback(model_path=model_path)\n",
    "    model.save(os.path.join('results', 'PPO', 'final_model_ppo'))\n",
    "    print(\"Training completed. Model saved in results/PPO/\")\n",
    "    print(\"TensorBoard logs saved in results/PPO/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BopTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
