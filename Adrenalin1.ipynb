{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN \n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.insert(0,'boptestGymService')\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import BoptestGymEnvRewardWeightCost, NormalizedActionWrapper, NormalizedObservationWrapper, SaveAndTestCallback,DiscretizedActionWrapper\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC,PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import requests\n",
    "url = 'http://127.0.0.1:5000'\n",
    "# url=\"https://api.boptest.net\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "    \n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get('cost_tot', 0) or 0\n",
    "        pdih_tot = kpis.get('pdih_tot', 0) or 0\n",
    "        pele_tot = kpis.get('pele_tot', 0) or 0\n",
    "        tdis_tot = kpis.get('tdis_tot', 0) or 0\n",
    "        idis_tot = kpis.get('idis_tot', 0) or 0\n",
    "\n",
    "        objective = (\n",
    "            cost_tot +\n",
    "            4.25 * (pdih_tot + pele_tot) +\n",
    "            0.005 * tdis_tot +\n",
    "            0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "        try:\n",
    "            #use this one running on local server\n",
    "            kpis = requests.get(f'{self.url}/kpi').json()['payload']\n",
    "\n",
    "            #use this when running boptest server\n",
    "            # print(self.test_id)\n",
    "            # print(self.url)\n",
    "            # kpis = requests.get('{0}/kpi/{1}'.format(self.url,self.testid)).json()['payload']\n",
    "            # print(kpis)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching KPIs: {e}\")\n",
    "            return 0  # In case of error, return zero reward\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "        \n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "        if current_objective==self.objective_integrand or  reward==-0.0:\n",
    "            print(\"Lottery\")\n",
    "            print(\"prev\",self.objective_integrand)\n",
    "            print(\"curr\",current_objective)\n",
    "            \n",
    "            reward=1\n",
    "            print(\"reward\",reward)\n",
    "            return reward\n",
    "        \n",
    "        print(\"prev\",self.objective_integrand)\n",
    "        print(\"curr\",current_objective)\n",
    "        print(\"reward\",reward)\n",
    "        self.objective_integrand = current_objective\n",
    "        \n",
    "        \n",
    "        return reward\n",
    "        # self.step_count += 1\n",
    "\n",
    "        # if self.objective_integrand is not None:\n",
    "        #     # Calculate the increase in objective\n",
    "        #     increase = current_objective - self.objective_integrand\n",
    "        #     print(\"current objective\",current_objective)\n",
    "        #     print(\"previos objective\",self.objective_integrand)\n",
    "        #     # Update cumulative increase\n",
    "        #     self.cumulative_increase += abs(increase)\n",
    "\n",
    "        #     # Calculate average absolute increase up to this point\n",
    "        #     average_increase = self.cumulative_increase / self.step_count\n",
    "\n",
    "        #     # Calculate relative performance (closer to 0 is better)\n",
    "        #     relative_performance = increase / (average_increase + self.epsilon)\n",
    "\n",
    "        #     # Transform relative performance to a reward between -1 and 1\n",
    "        #     reward = -np.tanh(relative_performance)\n",
    "\n",
    "        #     # Add a small positive reward if increase is less than average\n",
    "        #     if abs(increase) < average_increase:\n",
    "        #         reward += 0.1\n",
    "\n",
    "        #     # Add a larger reward if we managed to decrease the objective\n",
    "        #     if increase < 0:\n",
    "        #         reward += 0.5\n",
    "\n",
    "        # else:\n",
    "        #     reward = 0\n",
    "        # print(\"reward\",reward)\n",
    "        # # Update the stored previous objective\n",
    "        # self.objective_integrand = current_objective\n",
    "\n",
    "        # return reward\n",
    "    # def reset(self):\n",
    "    #     self.previous_objective = None\n",
    "    #     self.cumulative_increase = 0\n",
    "    #     self.step_count = 0\n",
    "    #     return super().reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (416823260.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    predictive_period     = 5*3600\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "def train_PPO_with_callback(model_path=None,\n",
    "                            log_dir=os.path.join('results', 'PPO_AD1', 'Model5'),\n",
    "                            tensorboard_log=os.path.join('results', 'PPO_AD1', 'Model3')):\n",
    "    \"\"\"\n",
    "    Method to train a PPO agent using a callback to save the model periodically.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str, optional\n",
    "        Path to a pre-trained model. If provided, the model will be loaded and further trained.\n",
    "    log_dir : str\n",
    "        Directory where monitoring data and best-trained model are stored.\n",
    "    tensorboard_log : str\n",
    "        Path to directory to load tensorboard logs.\n",
    "    \"\"\"\n",
    "    \n",
    "    excluding_periods = []\n",
    "    excluding_periods.append((173*24*3600, 266*24*3600))  # Summer period\n",
    "\n",
    "    env = BoptestGymEnvCustomReward(\n",
    "        url=url,\n",
    "        actions=['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        observations={\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
    "            'LowerSetp[1]':(280.,310.),\n",
    "            'UpperSetp[1]':(280.,310.),\n",
    "        },\n",
    "        predictive_period     = 5*3600,\n",
    "        scenario={'electricity_price': 'highly_dynamic'},\n",
    "        random_start_time=True,\n",
    "        max_episode_length=3*24*3600,\n",
    "        step_period=3600,\n",
    "        log_dir=tensorboard_log,\n",
    "        excluding_periods=excluding_periods\n",
    "    )\n",
    "\n",
    "    print(env.observation_space)\n",
    "    # env = NormalizedObservationWrapper(env)\n",
    "    # env = NormalizedActionWrapper(env)  \n",
    "    env = DiscretizedActionWrapper(env,n_bins_act=15)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    env = Monitor(env=env, filename=os.path.join(log_dir, 'monitor.csv'))\n",
    "    \n",
    "    # Callback to save model every 2000 steps\n",
    "    callback = SaveAndTestCallback(check_freq=48,save_freq=500,env=env,log_dir=tensorboard_log)\n",
    "    \n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Load existing model if model_path is given, else create a new one\n",
    "    if model_path and os.path.isfile(model_path):\n",
    "        model = PPO.load(model_path, env=env, tensorboard_log=tensorboard_log)\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        model.set_logger(new_logger)  # Reconfigure the logger to continue logging\n",
    "    else:\n",
    "        model = PPO(\n",
    "            'MlpPolicy', \n",
    "            env, \n",
    "            verbose=1, \n",
    "            gamma=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.2,\n",
    "            gae_lambda=0.95,\n",
    "            ent_coef=0.01,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "        )\n",
    "        model.set_logger(new_logger)\n",
    "        print(\"Starting training from scratch.\")\n",
    "    \n",
    "    # Train the agent with the callback\n",
    "    model.learn(total_timesteps=int(500000), callback=callback)\n",
    "    \n",
    "    return env, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"results/PPO_AD1/Model5/model_1.58m_latest.zip\" # Update this with the correct path if needed\n",
    "    env, model = train_PPO_with_callback(model_path=model_path)\n",
    "    model.save(os.path.join('results', 'PPO', 'final_model'))\n",
    "    print(\"Training completed. Model saved in results/PPO/\")\n",
    "    print(\"TensorBoard logs saved in results/PPO/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BopTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
