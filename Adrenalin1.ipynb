{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN \n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.insert(0,'boptestGymService')\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import BoptestGymEnvRewardWeightCost, NormalizedActionWrapper, NormalizedObservationWrapper, SaveAndTestCallback,DiscretizedActionWrapper\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC,PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import torch\n",
    "import wandb\n",
    "import requests\n",
    "url = 'http://127.0.0.1:5000'\n",
    "# url=\"https://api.boptest.net\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "    \n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get('cost_tot', 0) or 0\n",
    "        pdih_tot = kpis.get('pdih_tot', 0) or 0\n",
    "        pele_tot = kpis.get('pele_tot', 0) or 0\n",
    "        tdis_tot = kpis.get('tdis_tot', 0) or 0\n",
    "        idis_tot = kpis.get('idis_tot', 0) or 0\n",
    "\n",
    "        objective = (\n",
    "            cost_tot +\n",
    "            4.25 * (pdih_tot + pele_tot) +\n",
    "            0.005 * tdis_tot +\n",
    "            0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "        try:\n",
    "            #use this one running on local server\n",
    "            kpis = requests.get(f'{self.url}/kpi').json()['payload']\n",
    "\n",
    "            #use this when running boptest server\n",
    "            # print(self.test_id)\n",
    "            # print(self.url)\n",
    "            # kpis = requests.get('{0}/kpi/{1}'.format(self.url,self.testid)).json()['payload']\n",
    "            # print(kpis)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching KPIs: {e}\")\n",
    "            return 0  # In case of error, return zero reward\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "        \n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "        print(\"prev\",self.objective_integrand)\n",
    "        print(\"curr\",current_objective)\n",
    "        print(\"reward\",reward)\n",
    "        self.objective_integrand = current_objective\n",
    "        \n",
    "        \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With weight and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ggm8beyo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>1536</td></tr><tr><td>rollout/ep_len_mean</td><td>120</td></tr><tr><td>rollout/ep_rew_mean</td><td>-2.13094</td></tr><tr><td>time/fps</td><td>2</td></tr><tr><td>train/approx_kl</td><td>0.06884</td></tr><tr><td>train/clip_fraction</td><td>0.73848</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-9.12562</td></tr><tr><td>train/explained_variance</td><td>0</td></tr><tr><td>train/learning_rate</td><td>0.0003</td></tr><tr><td>train/loss</td><td>-0.24935</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.13038</td></tr><tr><td>train/value_loss</td><td>0.02218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">discrete_action_20</strong> at: <a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training/runs/ggm8beyo' target=\"_blank\">https://wandb.ai/dixitaniket1212-coventry-university/ppo-training/runs/ggm8beyo</a><br/> View project at: <a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training' target=\"_blank\">https://wandb.ai/dixitaniket1212-coventry-university/ppo-training</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>results/PPO_AD1/Model6/wandb/run-20240819_144921-ggm8beyo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ggm8beyo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>results/PPO_AD1/Model6/wandb/run-20240819_145320-ggm8beyo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training/runs/ggm8beyo' target=\"_blank\">discrete_action_20</a></strong> to <a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training' target=\"_blank\">https://wandb.ai/dixitaniket1212-coventry-university/ppo-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dixitaniket1212-coventry-university/ppo-training/runs/ggm8beyo' target=\"_blank\">https://wandb.ai/dixitaniket1212-coventry-university/ppo-training/runs/ggm8beyo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggm8beyo\n",
      "Logging to results/PPO_AD1/Model6\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Observation spaces do not match: Box([  0.  200.  200.  250.   -0.4  -0.4  -0.4  -0.4  -0.4  -0.4 280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.    0.    0.\n   0.    0.    0.    0. ], [3.1536e+07 4.0000e+02 2.0000e+03 3.5000e+02 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04], (28,), float32) != Box([  0.  200.  200.  250.   -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4\n  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4\n  -0.4 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.    0.    0.    0.    0.    0.\n   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n   0.    0.    0.    0. ], [3.1536e+07 4.0000e+02 2.0000e+03 3.5000e+02 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04], (88,), float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 152\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    151\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel6/ppo_ggm8beyo/model.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Update this with the correct path if needed\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     env, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_PPO_with_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model_ppo\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Model saved in results/PPO/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 119\u001b[0m, in \u001b[0;36mtrain_PPO_with_callback\u001b[0;34m(model_path, log_dir, tensorboard_log)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Check if CUDA is available and force GPU usage if possible\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Load existing model if model_path is given, else create a new one\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(model_path):\n\u001b[0;32m--> 119\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded pre-trained model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)  \u001b[38;5;66;03m# Reconfigure the logger to continue logging\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boptestlocal/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:717\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_env(env, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# Check if given env is valid\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m \u001b[43mcheck_for_correct_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Discard `_last_obs`, this will force the env to reset before training\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# See issue https://github.com/DLR-RM/stable-baselines3/issues/597\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_reset \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/boptestlocal/lib/python3.10/site-packages/stable_baselines3/common/utils.py:231\u001b[0m, in \u001b[0;36mcheck_for_correct_spaces\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mChecks that the environment has same spaces as provided ones. Used by BaseAlgorithm to check if\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03mspaces match after loading the model with given env.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m:param action_space: Action space to check against\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observation_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_space \u001b[38;5;241m!=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction spaces do not match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Observation spaces do not match: Box([  0.  200.  200.  250.   -0.4  -0.4  -0.4  -0.4  -0.4  -0.4 280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.    0.    0.\n   0.    0.    0.    0. ], [3.1536e+07 4.0000e+02 2.0000e+03 3.5000e+02 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04], (28,), float32) != Box([  0.  200.  200.  250.   -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4\n  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4  -0.4\n  -0.4 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.  280.\n 280.  280.  280.  280.  280.  280.  280.    0.    0.    0.    0.    0.\n   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n   0.    0.    0.    0. ], [3.1536e+07 4.0000e+02 2.0000e+03 3.5000e+02 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01 4.0000e-01\n 4.0000e-01 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02 3.1000e+02\n 3.1000e+02 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04\n 1.0000e+04 1.0000e+04 1.0000e+04 1.0000e+04], (88,), float32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "\n",
    "def train_PPO_with_callback(model_path=None,\n",
    "                            log_dir=os.path.join('results', 'PPO_AD1', 'Model6'),\n",
    "                            tensorboard_log=os.path.join('results', 'PPO_AD1', 'Model6')):\n",
    "    \"\"\"\n",
    "    Method to train a PPO agent using a callback to save the model periodically.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str, optional\n",
    "        Path to a pre-trained model. If provided, the model will be loaded and further trained.\n",
    "    log_dir : str\n",
    "        Directory where monitoring data and best-trained model are stored.\n",
    "    tensorboard_log : str\n",
    "        Path to directory to load tensorboard logs.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "      \n",
    "    excluding_periods = []\n",
    "    excluding_periods.append((173*24*3600, 266*24*3600))  # Summer period\n",
    "    env_config = {\n",
    "        'url': url,\n",
    "        'actions': ['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        'observations': {\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
    "            'LowerSetp[1]':(280.,310.),\n",
    "            'UpperSetp[1]':(280.,310.),\n",
    "            'UpperCO2[1]':(0,10000),\n",
    "        },\n",
    "        'predictive_period': 5*3600,\n",
    "        'scenario': {'electricity_price': 'highly_dynamic'},\n",
    "        'random_start_time': True,\n",
    "        'max_episode_length': 3*24*3600,\n",
    "        'step_period': 3600,\n",
    "        'log_dir': log_dir,\n",
    "        'excluding_periods': excluding_periods\n",
    "    }\n",
    "    env = BoptestGymEnvCustomReward(\n",
    "        url=url,\n",
    "        actions=['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        observations={\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic':(-0.4,0.4),\n",
    "            'LowerSetp[1]':(280.,310.),\n",
    "            'UpperSetp[1]':(280.,310.),\n",
    "            'UpperCO2[1]':(0,10000)\n",
    "        },\n",
    "        predictive_period     = 5*3600,\n",
    "        scenario={'electricity_price': 'highly_dynamic'},\n",
    "        random_start_time=True,\n",
    "        max_episode_length=5*24*3600,\n",
    "        step_period=3600,\n",
    "        log_dir=tensorboard_log,\n",
    "        excluding_periods=excluding_periods\n",
    "    )\n",
    "    \n",
    "    env = DiscretizedActionWrapper(env, n_bins_act=20)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    run=wandb.init(\n",
    "        \n",
    "        project=\"ppo-training\",\n",
    "        config={\n",
    "            'env': env_config,\n",
    "            'verbose': 1,\n",
    "            'gamma': 0.99,\n",
    "            'learning_rate': 3e-4,\n",
    "            'n_steps': 2048,\n",
    "            'batch_size': 64,\n",
    "            'n_epochs': 10,\n",
    "            'clip_range': 0.2,\n",
    "            'gae_lambda': 0.95,\n",
    "            'ent_coef': 0.01,\n",
    "            'device': device,\n",
    "            'action_bins':20\n",
    "        },\n",
    "        dir=log_dir,\n",
    "        id=\"ggm8beyo\",\n",
    "        name=\"discrete_action_20\",\n",
    "        resume=\"allow\",\n",
    "        sync_tensorboard=True,\n",
    "    )\n",
    "    print(run.id)\n",
    "    env = Monitor(env=env, filename=os.path.join(log_dir, 'monitor.csv'))\n",
    "    \n",
    "    # Callback to save model every 2000 steps\n",
    "    # callback = SaveAndTestCallback(check_freq=48,save_freq=500,env=env,log_dir=tensorboard_log)\n",
    "    \n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Check if CUDA is available and force GPU usage if possible\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    # Load existing model if model_path is given, else create a new one\n",
    "    if model_path and os.path.isfile(model_path):\n",
    "        model = PPO.load(model_path, env=env, tensorboard_log=tensorboard_log, device=device)\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        model.set_logger(new_logger)  # Reconfigure the logger to continue logging\n",
    "    else:\n",
    "        model = PPO(\n",
    "            'MlpPolicy', \n",
    "            env, \n",
    "            verbose=1, \n",
    "            gamma=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=512,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            clip_range=0.2,\n",
    "            gae_lambda=0.95,\n",
    "            ent_coef=0.01,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            device=device\n",
    "        )\n",
    "        model.set_logger(new_logger)\n",
    "        print(\"Starting training from scratch.\")\n",
    "    \n",
    "    # Verify that the model is on the correct device\n",
    "    print(f\"Model is on device: {next(model.policy.parameters()).device}\")\n",
    "    \n",
    "    # Train the agent with the callback\n",
    "    model.learn(total_timesteps=1000000, callback=WandbCallback(verbose=2,model_save_freq=1000,model_save_path=f\"Model6/ppo_{run.id}\",gradient_save_freq=100))\n",
    "    # Finish W&B logging\n",
    "    run.finish()\n",
    "    return env, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"Model6/ppo_ggm8beyo/model.zip\" # Update this with the correct path if needed\n",
    "    env, model = train_PPO_with_callback(model_path=model_path)\n",
    "    model.save(os.path.join('results', 'PPO', 'final_model_ppo'))\n",
    "    print(\"Training completed. Model saved in results/PPO/\")\n",
    "    print(\"TensorBoard logs saved in results/PPO/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BopTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
