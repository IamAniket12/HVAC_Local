{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN \n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.insert(0,'boptestGymService')\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import BoptestGymEnvRewardWeightCost, NormalizedActionWrapper, NormalizedObservationWrapper, SaveAndTestCallback,DiscretizedActionWrapper\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC,PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import requests\n",
    "url = 'http://127.0.0.1:5000'\n",
    "# url=\"https://api.boptest.net\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "    \n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get('cost_tot')\n",
    "        pdih_tot = kpis.get('pdih_tot') \n",
    "        pele_tot = kpis.get('pele_tot') \n",
    "        tdis_tot = kpis.get('tdis_tot') \n",
    "        idis_tot = kpis.get('idis_tot')\n",
    "\n",
    "        objective = (\n",
    "            cost_tot +\n",
    "            4.25 * (pdih_tot + pele_tot) +\n",
    "            0.005 * tdis_tot +\n",
    "            0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "        #use this one running on local server\n",
    "        kpis = requests.get(f'{self.url}/kpi').json()['payload']\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "        \n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "        print(reward)\n",
    "        self.objective_integrand = current_objective\n",
    "        \n",
    "        \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "def train_A2C_with_callback(model_path=None,\n",
    "                            log_dir=os.path.join('results', 'A2C_AD1', 'Model2'),\n",
    "                            tensorboard_log=os.path.join('results', 'A2C_AD1', 'Model2')):\n",
    "    \"\"\"\n",
    "    Method to train an A2C agent using a callback to save the model periodically and log to WandB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str, optional\n",
    "        Path to a pre-trained model. If provided, the model will be loaded and further trained.\n",
    "    log_dir : str\n",
    "        Directory where monitoring data and best-trained model are stored.\n",
    "    tensorboard_log : str\n",
    "        Path to directory to load tensorboard logs.\n",
    "    \"\"\"\n",
    "    \n",
    "    excluding_periods = []\n",
    "    excluding_periods.append((173*24*3600, 266*24*3600))  # Summer period\n",
    "\n",
    "    # Define environment configuration\n",
    "    env_config = {\n",
    "        \"url\": url,\n",
    "        \"actions\": ['ahu_oveFanSup_u','oveValCoi_u', 'oveValRad_u'], \n",
    "        \"observations\": {\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic': (-0.4, 0.4),\n",
    "            'LowerSetp[1]': (280., 310.),\n",
    "            'UpperSetp[1]': (280., 310.),\n",
    "            'UpperCO2[1]':(0,10000)\n",
    "        },\n",
    "        \"predictive_period\": 24 * 3600,\n",
    "        \"scenario\": {'electricity_price': 'highly_dynamic'},\n",
    "        \"random_start_time\": True,\n",
    "        \"max_episode_length\": 3 * 24 * 3600,\n",
    "        \"step_period\": 3600,\n",
    "        \"excluding_periods\": excluding_periods\n",
    "    }\n",
    "\n",
    "    env = BoptestGymEnvCustomReward(**env_config)\n",
    "    env = DiscretizedActionWrapper(env, n_bins_act=6)\n",
    "    # env = NormalizedObservationWrapper(env)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "        print(\"CUDA is available. Using GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    env = Monitor(env=env, filename=os.path.join(log_dir, 'monitor.csv'))\n",
    "\n",
    "    # Initialize WandB and start a new run, with environment configuration added\n",
    "    run = wandb.init(\n",
    "        project=\"A2C-Training\",  \n",
    "        sync_tensorboard=True,  \n",
    "        config={\n",
    "            \"algo\": \"A2C\",\n",
    "            \"total_timesteps\": 1000000,\n",
    "            \"gamma\": 0.99,\n",
    "            \"learning_rate\": 7e-4,  \n",
    "            \"n_steps\": 5,  \n",
    "            \"vf_coef\": 0.25,  \n",
    "            \"ent_coef\": 0.01,  \n",
    "            \"env_config\": env_config  \n",
    "        },\n",
    "        name=\"A2C_6bins\",\n",
    "        id=\"14\",\n",
    "        resume=\"allow\"\n",
    "    )\n",
    "    print(run.id)\n",
    "    print(env.action_space)\n",
    "    # Callback to save model every 1000 steps with a unique name\n",
    "    # checkpoint_callback = CheckpointCallback(\n",
    "    #     save_freq=2000, \n",
    "    #     save_path=log_dir,\n",
    "    #     name_prefix=\"a2c_model\",\n",
    "    #     verbose=1\n",
    "    # )\n",
    "    callback = SaveAndTestCallback(env,check_freq=1000,save_freq=1000,log_dir=log_dir,test=False)\n",
    "\n",
    "    # WandB callback to track training metrics\n",
    "    wandb_callback = WandbCallback(\n",
    "        model_save_path=os.path.join(log_dir, \"wandb_models\"),\n",
    "        model_save_freq=1000,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "\n",
    "    # Load existing model if model_path is given, else create a new one\n",
    "    if model_path and os.path.isfile(model_path):\n",
    "        model = A2C.load(model_path, env=env, tensorboard_log=tensorboard_log)\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        model.set_logger(new_logger)\n",
    "    else:\n",
    "        model = A2C(\n",
    "            'MlpPolicy', \n",
    "            env, \n",
    "            verbose=1, \n",
    "            gamma=0.99,\n",
    "            learning_rate=7e-4,  \n",
    "            n_steps=5,  \n",
    "            vf_coef=0.25,  \n",
    "            ent_coef=0.01,  \n",
    "            tensorboard_log=tensorboard_log,\n",
    "        )\n",
    "        model.set_logger(new_logger)\n",
    "        print(\"Starting training from scratch.\")\n",
    "     # Verify that the model is on the correct device\n",
    "    print(f\"Model is on device: {next(model.policy.parameters()).device}\")\n",
    "\n",
    "    # Train the agent with the callback\n",
    "    model.learn(total_timesteps=int(1000000), callback=[callback, wandb_callback])\n",
    "    \n",
    "    # Finish WandB run\n",
    "    run.finish()\n",
    "    \n",
    "    return env, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = None\n",
    "    env, model = train_A2C_with_callback(model_path=model_path)\n",
    "    model.save(os.path.join('results', 'A2C', 'final_model'))\n",
    "    print(\"Training completed. Model saved in results/A2C/\")\n",
    "    print(\"TensorBoard logs saved in results/A2C/\")\n",
    "    print(\"done1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boptestlocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
