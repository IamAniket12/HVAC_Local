{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN \n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "sys.path.insert(0,'boptestGymService')\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import BoptestGymEnvRewardWeightCost, NormalizedActionWrapper, NormalizedObservationWrapper, SaveAndTestCallback,DiscretizedActionWrapper\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC,PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "import requests\n",
    "url = 'http://127.0.0.1:5000'\n",
    "# url=\"https://api.boptest.net\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "    \n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get('cost_tot', 0) or 0\n",
    "        pdih_tot = kpis.get('pdih_tot', 0) or 0\n",
    "        pele_tot = kpis.get('pele_tot', 0) or 0\n",
    "        tdis_tot = kpis.get('tdis_tot', 0) or 0\n",
    "        idis_tot = kpis.get('idis_tot', 0) or 0\n",
    "\n",
    "        objective = (\n",
    "            cost_tot +\n",
    "            4.25 * (pdih_tot + pele_tot) +\n",
    "            0.005 * tdis_tot +\n",
    "            0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "        try:\n",
    "            #use this one running on local server\n",
    "            kpis = requests.get(f'{self.url}/kpi').json()['payload']\n",
    "\n",
    "            #use this when running boptest server\n",
    "            # print(self.test_id)\n",
    "            # print(self.url)\n",
    "            # kpis = requests.get('{0}/kpi/{1}'.format(self.url,self.testid)).json()['payload']\n",
    "            # print(kpis)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching KPIs: {e}\")\n",
    "            return 0  # In case of error, return zero reward\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "        \n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "       \n",
    "        self.objective_integrand = current_objective\n",
    "        \n",
    "        \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.callbacks import WandbCallback\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "def train_SAC_with_callback(model_path=None,\n",
    "                            log_dir=os.path.join('results', 'SAC_AD1', 'Model1'),\n",
    "                            tensorboard_log=os.path.join('results', 'SAC_AD1', 'Model1')):\n",
    "    \"\"\"\n",
    "    Method to train a SAC agent using a callback to save the model periodically and log to WandB.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str, optional\n",
    "        Path to a pre-trained model. If provided, the model will be loaded and further trained.\n",
    "    log_dir : str\n",
    "        Directory where monitoring data and best-trained model are stored.\n",
    "    tensorboard_log : str\n",
    "        Path to directory to load tensorboard logs.\n",
    "    \"\"\"\n",
    "    \n",
    "    excluding_periods = []\n",
    "    excluding_periods.append((173*24*3600, 266*24*3600))  # Summer period\n",
    "\n",
    "    # Define environment configuration\n",
    "    env_config = {\n",
    "        \"url\": url,\n",
    "        \"actions\": ['ahu_oveFanSup_u', 'oveValCoi_u', 'oveValRad_u'],\n",
    "        \"observations\": {\n",
    "            'time': (0, 31536000),\n",
    "            'reaTZon_y': (200., 400.),\n",
    "            'reaCO2Zon_y': (200., 2000.),\n",
    "            'weaSta_reaWeaTDryBul_y': (250., 350.),\n",
    "            'PriceElectricPowerHighlyDynamic': (-0.4, 0.4),\n",
    "            'LowerSetp[1]': (280., 310.),\n",
    "            'UpperSetp[1]': (280., 310.),\n",
    "            'UpperCO2[1]':(0,10000)\n",
    "        },\n",
    "        \"predictive_period\": 5 * 3600,\n",
    "        \"scenario\": {'electricity_price': 'highly_dynamic'},\n",
    "        \"random_start_time\": True,\n",
    "        \"max_episode_length\": 3 * 24 * 3600,\n",
    "        \"step_period\": 3600,\n",
    "        \"excluding_periods\": excluding_periods\n",
    "    }\n",
    "\n",
    "    env = BoptestGymEnvCustomReward(**env_config)\n",
    "\n",
    "    \n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    env = Monitor(env=env, filename=os.path.join(log_dir, 'monitor.csv'))\n",
    "\n",
    "    # Initialize WandB and start a new run, with environment configuration added\n",
    "    run = wandb.init(\n",
    "        project=\"SAC-Training\",  # Replace with your project name\n",
    "          # Replace with your WandB entity\n",
    "        sync_tensorboard=True,  # Auto-sync with TensorBoard\n",
    "        config={\n",
    "            \"algo\": \"SAC\",\n",
    "              # Replace with the actual environment name\n",
    "            \"total_timesteps\": 1000000,\n",
    "            \"gamma\": 0.99,\n",
    "            \"learning_rate\": 3e-4,\n",
    "            \"buffer_size\": 1000000,\n",
    "            \"batch_size\": 256,\n",
    "            \"train_freq\": 1,\n",
    "            \"gradient_steps\": 1,\n",
    "            \"ent_coef\": 'auto',\n",
    "            \"env_config\": env_config  # Include the environment configuration\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Callback to save model every 1000 steps with a unique name\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=1000, \n",
    "        save_path=log_dir,\n",
    "        name_prefix=\"sac_model\",\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # WandB callback to track training metrics\n",
    "    wandb_callback = WandbCallback(\n",
    "        model_save_path=os.path.join(log_dir, \"wandb_models\"),\n",
    "        model_save_freq=1000,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "\n",
    "    # Load existing model if model_path is given, else create a new one\n",
    "    if model_path and os.path.isfile(model_path):\n",
    "        model = SAC.load(model_path, env=env, tensorboard_log=tensorboard_log)\n",
    "        print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        model.set_logger(new_logger)  # Reconfigure the logger to continue logging\n",
    "    else:\n",
    "        model = SAC(\n",
    "            'MlpPolicy', \n",
    "            env, \n",
    "            verbose=1, \n",
    "            gamma=0.99,\n",
    "            learning_rate=3e-4,\n",
    "            buffer_size=1000000,\n",
    "            batch_size=256,\n",
    "            train_freq=1,\n",
    "            gradient_steps=1,\n",
    "            ent_coef='auto',\n",
    "            tensorboard_log=tensorboard_log,\n",
    "        )\n",
    "        model.set_logger(new_logger)\n",
    "        print(\"Starting training from scratch.\")\n",
    "    \n",
    "    # Train the agent with the callback\n",
    "    model.learn(total_timesteps=int(100000), callback=[checkpoint_callback, wandb_callback])\n",
    "    \n",
    "    # Finish WandB run\n",
    "    run.finish()\n",
    "    \n",
    "    return env, model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = None # Update this with the correct path if needed\n",
    "    env, model = train_SAC_with_callback(model_path=model_path)\n",
    "    model.save(os.path.join('results', 'SAC', 'final_model'))\n",
    "    print(\"Training completed. Model saved in results/SAC/\")\n",
    "    print(\"TensorBoard logs saved in results/SAC/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boptestlocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
