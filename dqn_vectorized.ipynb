{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from stable_baselines3 import DQN\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.insert(0, \"boptestGymService\")\n",
    "from boptestGymEnv import BoptestGymEnv\n",
    "from boptestGymEnv import (\n",
    "    BoptestGymEnvRewardWeightCost,\n",
    "    NormalizedActionWrapper,\n",
    "    NormalizedObservationWrapper,\n",
    "    SaveAndTestCallback,\n",
    "    DiscretizedActionWrapper,\n",
    ")\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from testing import utilities\n",
    "import random\n",
    "import os\n",
    "from stable_baselines3 import SAC, PPO, TD3\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "from stable_baselines3 import TD3  # Import TD3 instead of SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from wandb.integration.sb3 import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "\n",
    "class BoptestGymEnvCustomReward(BoptestGymEnv):\n",
    "\n",
    "    def calculate_objective(self, kpis):\n",
    "        \"\"\"\n",
    "        Calculate the objective based on the given KPI values.\n",
    "        \"\"\"\n",
    "        cost_tot = kpis.get(\"cost_tot\")\n",
    "        pdih_tot = kpis.get(\"pdih_tot\")\n",
    "        pele_tot = kpis.get(\"pele_tot\")\n",
    "        tdis_tot = kpis.get(\"tdis_tot\")\n",
    "        idis_tot = kpis.get(\"idis_tot\")\n",
    "\n",
    "        objective = (\n",
    "            cost_tot\n",
    "            + 4.25 * (pdih_tot + pele_tot)\n",
    "            + 0.005 * tdis_tot\n",
    "            + 0.0001 * idis_tot\n",
    "        )\n",
    "\n",
    "        return objective\n",
    "\n",
    "    def get_reward(self):\n",
    "\n",
    "        kpis = requests.get(f\"{self.url}/kpi\").json()[\"payload\"]\n",
    "\n",
    "        current_objective = self.calculate_objective(kpis)\n",
    "        # Compute reward\n",
    "\n",
    "        reward = -(current_objective - self.objective_integrand)\n",
    "        print(\"reward\", reward)\n",
    "        self.objective_integrand = current_objective\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from testing import utilities\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env.vec_monitor import VecMonitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from boptestGymEnv import (\n",
    "    BoptestGymEnv,\n",
    "    NormalizedObservationWrapper,\n",
    "    DiscretizedActionWrapper,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_urls_from_yml(boptest_root_dir):\n",
    "    \"\"\"Method that returns as many urls for BOPTEST-Gym environments\n",
    "    as those specified at the BOPTEST `docker-compose.yml` file.\n",
    "    It assumes that `generateDockerComposeYml.py` has been called first.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    boptest_root_dir: str\n",
    "        String with directory to BOPTEST where the `docker-compose.yml`\n",
    "        file should be located.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    urls: list\n",
    "        List of urls where BOPTEST test cases will be allocated.\n",
    "\n",
    "    \"\"\"\n",
    "    docker_compose_loc = os.path.join(boptest_root_dir, \"docker-compose.yml\")\n",
    "\n",
    "    # Read the docker-compose.yml file\n",
    "    with open(docker_compose_loc, \"r\") as stream:\n",
    "        try:\n",
    "            docker_compose_data = yaml.safe_load(stream)\n",
    "            services = docker_compose_data.get(\"services\", {})\n",
    "\n",
    "            # Extract the port and URL of the service\n",
    "            urls = []\n",
    "            for service, config in services.items():\n",
    "                ports = config.get(\"ports\", [])\n",
    "                for port in ports:\n",
    "                    # Extract host port\n",
    "                    host_port = port.split(\":\")[1]\n",
    "                    urls.append(f\"http://127.0.0.1:{host_port}\")\n",
    "\n",
    "            print(urls)  # Print URLs\n",
    "\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def make_env(url):\n",
    "    \"\"\"Function that instantiates the environment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        Rest API url for communication with this environment.\n",
    "    seed: integer\n",
    "        Seed for random starting times of episodes in this environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def _init():\n",
    "     \n",
    "        env = BoptestGymEnvCustomReward(\n",
    "            url=url,\n",
    "            actions=[\"ahu_oveFanSup_u\", \"oveValCoi_u\", \"oveValRad_u\"],\n",
    "            observations={\n",
    "                \"time\": (0, 604800),\n",
    "                \"reaTZon_y\": (280.0, 310.0),\n",
    "                \"reaCO2Zon_y\": (200.0, 2000.0),\n",
    "                \"PriceElectricPowerHighlyDynamic\": (-0.4, 0.4),\n",
    "                \"LowerSetp[1]\": (280.0, 310.0),\n",
    "                \"UpperSetp[1]\": (280.0, 310.0),\n",
    "            },\n",
    "            scenario={\"electricity_price\": \"dynamic\"},\n",
    "            predictive_period=4 * 3600,\n",
    "            random_start_time=True,\n",
    "            excluding_periods=[\n",
    "                (22 * 24 * 3600, 265 * 24 * 3600),\n",
    "                (290 * 24 * 3600, 365 * 24 * 3600),\n",
    "            ],\n",
    "            max_episode_length=3 * 3600,\n",
    "            step_period=1800,\n",
    "        )\n",
    "        env = NormalizedObservationWrapper(\n",
    "            env\n",
    "        )  # Add observation normalization if needed\n",
    "        env = DiscretizedActionWrapper(\n",
    "            env, n_bins_act=10\n",
    "        )  # Add action discretization if needed\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _init\n",
    "\n",
    "\n",
    "def train_DQN_vectorized(\n",
    "    venv,\n",
    "    log_dir=os.path.join('results', 'DQN_AD1', 'Model3'),\n",
    "    tensorboard_log=os.path.join('results', 'DQN_AD1', 'Model3')\n",
    "):\n",
    "    \"\"\"Method to train DQN agent using vectorized environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    venv: stable_baselines3.common.vec_env.SubprocVecEnv\n",
    "        vectorized environment to be learned.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create logging directory if not exists. Monitoring data and agent model will be stored here\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    env_config = { \"url\": \"url\",\n",
    "                \"actions\": [\"ahu_oveFanSup_u\", \"oveValCoi_u\", \"oveValRad_u\"],\n",
    "                \"observations\": {\n",
    "                    \"time\": [0, 604800],\n",
    "                    \"reaTZon_y\": [280.0, 310.0],\n",
    "                    \"reaCO2Zon_y\": [200.0, 2000.0],\n",
    "                    \"PriceElectricPowerHighlyDynamic\": [-0.4, 0.4],\n",
    "                    \"LowerSetp[1]\": [280.0, 310.0],\n",
    "                    \"UpperSetp[1]\": [280.0, 310.0],\n",
    "                },\n",
    "                \"scenario\": {\"electricity_price\": \"dynamic\"},\n",
    "                \"predictive_period\": 14400,\n",
    "                \"random_start_time\": \"true\",\n",
    "                \"excluding_periods\": [[1900800, 22896000], [25056000, 31536000]],\n",
    "                \"max_episode_length\": 10800,\n",
    "                \"step_period\": 1800,}\n",
    "    # Modify the environment to include the callback\n",
    "    venv = VecMonitor(venv=venv, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "    run = wandb.init(\n",
    "        project=\"DQN\",  # Replace with your project name\n",
    "        sync_tensorboard=True,  # Auto-sync with TensorBoard\n",
    "        config=env_config,\n",
    "        name=\"DQN\",\n",
    "        id=\"4\",\n",
    "        resume=\"allow\",\n",
    "\n",
    "    )\n",
    "    print(run.id)\n",
    "    # Create the callback: evaluate with one episode after 100 steps for training. We keep it very short for testing.\n",
    "    # When using multiple environments, each call to ``env.step()`` will effectively correspond to ``n_envs`` steps.\n",
    "    # To account for that, you can use ``eval_freq = eval_freq/venv.num_envs``\n",
    "    eval_freq = 1000\n",
    "    eval_callback = EvalCallback(\n",
    "        venv,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=int(eval_freq / venv.num_envs),\n",
    "        n_eval_episodes=1,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    wandb_callback = WandbCallback(\n",
    "        model_save_path=log_dir,\n",
    "        model_save_freq=1000,\n",
    "        verbose=2,\n",
    "        \n",
    "    )\n",
    "    # Try to find CUDA core since it's optimized for parallel computing tasks\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Instantiate an RL agent with DQN\n",
    "    # model = DQN(\n",
    "    #     \"MlpPolicy\",\n",
    "    #     venv,\n",
    "    #     verbose=1,\n",
    "    #     gamma=0.99,\n",
    "    #     learning_rate=5e-4,\n",
    "    #     batch_size=24,\n",
    "    #     seed=123456,\n",
    "    #     buffer_size=365 * 24,\n",
    "    #     learning_starts=24,\n",
    "    #     train_freq=1,\n",
    "    #     exploration_initial_eps=1.0,\n",
    "    #     exploration_final_eps=0.01,\n",
    "    #     exploration_fraction=0.1,\n",
    "    #     device=device,\n",
    "    #     tensorboard_log=tensorboard_log,\n",
    "    # )\n",
    "    model=DQN.load(\"results/DQN_AD1/Model3/best_model.zip\",env=venv,tensorboard_log=tensorboard_log)\n",
    "    \n",
    "    # Set up logger with TensorBoard logging continuation\n",
    "    new_logger = configure(log_dir, ['stdout', 'csv', 'tensorboard'])\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    # Main training loop\n",
    "    model.learn(total_timesteps=5000000, callback=[eval_callback,wandb_callback])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    boptest_root = \"/Users/aniketdixit/Desktop/ADRENALIN/HVAC_Local/boptestGymService\"\n",
    "\n",
    "    # Get the argument from command line when use Linux\n",
    "\n",
    "    boptest_root_dir = boptest_root\n",
    "\n",
    "    # Use URLs obtained from docker-compose.yml\n",
    "    urls = generate_urls_from_yml(boptest_root_dir=boptest_root_dir)\n",
    "\n",
    "    # Create BOPTEST-Gym environment replicas\n",
    "    envs = [make_env(url) for url in urls]\n",
    "\n",
    "    # Create a vectorized environment using SubprocVecEnv\n",
    "    venv = SubprocVecEnv(envs)\n",
    "\n",
    "    # Train vectorized environment\n",
    "    train_DQN_vectorized(venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BopTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
